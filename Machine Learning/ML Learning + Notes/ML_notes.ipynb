{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a breakdown of the key differences between scikit-learn (sklearn) and TensorFlow:\n",
    "\n",
    "scikit-learn (sklearn):\n",
    "\n",
    "- Purpose: A high-level library in Python designed for traditional machine learning tasks like classification, regression, clustering, dimensionality reduction, and model selection.\n",
    "- Focus: Ease of use, providing well-documented and efficient implementations of common machine learning algorithms.\n",
    "- Strengths:\n",
    "    - Simple API, ideal for beginners or quick prototyping.\n",
    "    - Fast performance for common algorithms on smaller datasets.\n",
    "    - Offers a wide range of built-in algorithms and utilities.\n",
    "    - Good for interpretable models, where you can understand how features contribute to predictions.\n",
    "- Weaknesses:\n",
    "    - Limited flexibility for building complex or custom neural network architectures.\n",
    "- May not handle very large datasets as efficiently as TensorFlow.\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "- Purpose: A powerful open-source library for numerical computation with a strong focus on deep learning. It can also be used for various scientific computing tasks.\n",
    "- Focus: Flexibility and performance for building and training complex models, especially deep neural networks.\n",
    "- Strengths:\n",
    "    - Highly customizable, allowing you to define intricate neural network architectures.\n",
    "    - Efficient for training large models on big datasets, often leveraging GPUs for acceleration.\n",
    "    - Supports various hardware platforms, including CPUs and GPUs.\n",
    "    - Can be used for other types of computations beyond machine learning.\n",
    "- Weaknesses:\n",
    "    - Lower-level API compared to scikit-learn, requiring more coding effort.\n",
    "    - Steeper learning curve, especially for beginners unfamiliar with deep learning concepts.\n",
    "    - Interpretability can be challenging for complex neural networks.\n",
    "\n",
    "Choosing the Right Tool:\n",
    "\n",
    "- For basic machine learning tasks or interpretable models: scikit-learn is an excellent choice due to its simplicity and ease of use.\n",
    "- For building complex deep learning models, especially with large datasets: TensorFlow's flexibility and performance make it a powerful option.\n",
    "\n",
    "Here's an analogy:\n",
    "\n",
    "- scikit-learn: Like a well-equipped toolbox with pre-built tools for common construction tasks. Easy to use but not ideal for building a skyscraper.\n",
    "- TensorFlow: Like a full machine shop with raw materials and tools. Requires more expertise but allows for constructing anything you can design, including complex structures.\n",
    "\n",
    "In some cases, you might consider using both together. For example, you could use scikit-learn for data preprocessing and then feed the prepared data into a TensorFlow model for deep learning.\n",
    "\n",
    "I hope this comprehensive explanation clarifies the distinctions between scikit-learn and TensorFlow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convoluted Network Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "In convolutional neural networks (CNNs), padding refers to a technique used to control the spatial dimensions (height and width) of the feature maps during convolution operations. There are three main types of padding:\n",
    "\n",
    "1. Same Padding:\n",
    "\n",
    "- This is the type of padding used in your code (padding='same').\n",
    "\n",
    "- Same padding aims to maintain the original height and width of the feature map after the convolution operation.\n",
    "\n",
    "- It achieves this by adding extra padding pixels around the borders of the input feature map. The number of padding pixels added depends on the filter size, padding type ('same' in this case), and strides used in the convolution layer.\n",
    "\n",
    "- Here's how it works:\n",
    "\n",
    "    - The number of padding pixels to add (both horizontally and vertically) is calculated using a formula that considers:\n",
    "\n",
    "        - Filter size (kernel size) - denoted by F\n",
    "        - Strides - denoted by S\n",
    "        - Output dimension (desired to be same as input in this case) - denoted by O\n",
    "        - Input dimension - denoted by I\n",
    "    - The formula to calculate padding (P) is: P = ((O - 1) * S + F - I) / 2\n",
    "\n",
    "- Benefits of same padding:\n",
    "\n",
    "    - It simplifies reasoning about the output size of the convolution operation.\n",
    "    - It allows you to focus on designing the network architecture without worrying about drastic changes in feature map sizes.\n",
    "\n",
    "2. Valid Padding:\n",
    "\n",
    "- Valid padding performs convolution without adding any padding pixels.\n",
    "- This can result in a smaller output feature map compared to the input.\n",
    "- The output size is calculated using a similar formula, but with P set to 0: O = (I - F + S) / S\n",
    "- While valid padding might lead to a smaller output, it can be useful when you want to gradually decrease the spatial dimensions of the feature maps as you go deeper into the network.\n",
    "\n",
    "3. No Padding (Zero Padding):\n",
    "\n",
    "- This is a special case where no padding pixels are added, but the output size is still calculated assuming same padding (using the formula with P=0).\n",
    "- This can lead to a smaller output size than both same padding and valid padding.\n",
    "- It's generally not recommended as it can lead to information loss at the edges of the feature maps.\n",
    "\n",
    "Choosing the Right Padding:\n",
    "\n",
    "- Same padding is a popular choice for many CNN architectures as it maintains the spatial dimensions, making it easier to reason about output sizes.\n",
    "- Valid padding can be used when you want to reduce the feature map size as you progress through the network (downsampling).\n",
    "- It's important to experiment and compare the performance of your model with different padding options to see what works best for your specific task and dataset.\n",
    "\n",
    "In your model, using padding='same' for all convolutional layers ensures that the output feature maps retain the same height and width as the input images throughout the initial convolutional stages. This can be beneficial for tasks like image classification where preserving spatial information is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, the model.build() method serves a crucial purpose in ensuring the compatibility and validity of your model's architecture. Here's a breakdown of its key functions:\n",
    "\n",
    "1. Layer Verification:\n",
    "\n",
    "- model.build() performs a check to make sure all the layers you've added to your model (using model.add()) are compatible with each other.\n",
    "- This involves verifying input and output shapes across layers. For example, the output shape of one layer must match the expected input shape of the next layer in the sequence.\n",
    "- If there are any inconsistencies, model.build() will raise an error, preventing you from proceeding with training or inference until the architecture is fixed.\n",
    "\n",
    "2. Internal Initialization:\n",
    "\n",
    "- When you call model.build(), it triggers the internal creation and initialization of necessary attributes within the model object.\n",
    "- Some of these attributes include:\n",
    "    - Internal weight tensors (parameters) for each layer that are essential for computations during training.\n",
    "    - Tracking information about layers and their connections.\n",
    "    - Buffers or caches for intermediate calculations (used in some layers or optimizers).\n",
    "- By explicitly building the model, you're ensuring that these internal structures are set up correctly for training and inference.\n",
    "\n",
    "3. Optional (But Recommended):\n",
    "\n",
    "- While calling model.build() isn't strictly mandatory in some cases, it's generally considered a good practice for the following reasons:\n",
    "    - Proactive Error Discovery: Catching shape mismatches or other architecture issues early on can save you debugging time later.\n",
    "    - Improved Performance: In specific scenarios, Keras might optimize certain internal computations based on the built model structure, potentially leading to slight performance gains.\n",
    "\n",
    "4. When is model.build() Necessary?\n",
    "\n",
    "- You might not always need to call model.build() explicitly in these situations:\n",
    "- If you're using a pre-built model (e.g., from TensorFlow Hub) with a defined input shape, Keras usually builds the model automatically when you first call a method like predict().\n",
    "- In simple models where the input shape is explicitly provided to all layers during creation (using the input_shape argument in model.add()), there might be a lower chance of shape mismatches.\n",
    "\n",
    "5. When is model.build() Important?\n",
    "\n",
    "- However, you're strongly encouraged to call model.build() in the following cases:\n",
    "    - When you define a model dynamically (e.g., building layers based on user input or other conditions).\n",
    "    - If you're working with complex architectures that involve custom layers or functional models.\n",
    "- As a general best practice for clarity and debugging, calling model.build() explicitly can help ensure your model is ready for training or prediction.\n",
    "\n",
    "In Summary:\n",
    "\n",
    "- model.build() acts as a safeguard and optimization tool in Keras models. It verifies compatibility, initializes internal structures, and promotes catching potential issues early on. While not always strictly necessary, it's highly recommended to build your model using model.build() before training or prediction, especially for more complex or dynamic architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Hub Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See 7 and 8 Tensorflow Hub for details."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
